{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import CaboCha\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.jupyter.settings import DATA_DIR, get_line\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set()\n",
    "plt.rcParams['font.family'] = 'IPAPGothic'  # TO-DO: Check detail and decide to remove or not\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10章: ベクトル空間法 (II)\n",
    "\n",
    "第10章では, 前章に引き続き単語ベクトルの学習に取り組む. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 90. word2vecによる学習\n",
    "\n",
    "81で作成したコーパスに対して [word2vec](https://code.google.com/p/word2vec/) を適用し, 単語ベクトルを学習せよ. さらに, 学習した単語ベクトルの形式を変換し, 86-89のプログラムを動かせ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 91. アナロジーデータの準備\n",
    "\n",
    "[単語アナロジーの評価データ](https://word2vec.googlecode.com/svn/trunk/questions-words.txt)をダウンロードせよ. このデータ中で \": \" で始まる行はセクション名を表す. 例えば, \": capital-common-countries\" という行は, \"capital-common-countries\" というセクションの開始を表している. ダウンロードした評価データの中で, \"family\" というセクションに含まれる評価事例を抜き出してファイルに保存せよ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 92. アナロジーデータへの適用\n",
    "\n",
    "[91](#91)で作成した評価データの各事例に対して, vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語) を計算し, そのベクトルと類似度が最も高い単語と, その類似度を求めよ. 求めた単語と類似度は, 各事例の末尾に追記せよ. このプログラムを85で作成した単語ベクトル, 90で作成した単語ベクトルに対して適用せよ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 93. アナロジータスクの正解率の計算\n",
    "\n",
    "[92](#92)で作ったデータを用い, 各モデルのアナロジータスクの正解率を求めよ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 94. WordSimilarity-353 での類似度計算\n",
    "\n",
    "[The WordSimilarity-353 Test Collection](http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/) の評価データを入力とし, 1列目と2列目の単語の類似度を計算し, 各行の末尾に類似度の値を追加するプログラムを作成せよ. このプログラムを85で作成した単語ベクトル, 90で作成した単語ベクトルに対して適用せよ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95. WordSimilarity-353 での評価\n",
    "\n",
    "[94](#94)で作ったデータを用い, 各モデルが出力する類似度のランキングと, 人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 96. 国名に関するベクトルの抽出\n",
    "\n",
    "word2vec の学習結果から, 国名に関するベクトルのみを抜き出せ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 97. k-means クラスタリング\n",
    "\n",
    "[96](#96)の単語ベクトルに対して, k-means クラスタリングをクラスタ数 $k=5$ として実行せよ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 98. Ward 法によるクラスタリング\n",
    "\n",
    "[96](#96)の単語ベクトルに対して, Ward 法による階層型クラスタリングを実行せよ. さらに, クラスタリング結果をデンドログラムとして可視化せよ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 99. t-SNE による可視化\n",
    "\n",
    "[96](#96)の単語ベクトルに対して, ベクトル空間を t-SNE で可視化せよ. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
