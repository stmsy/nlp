{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import MeCab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.jupyter.settings import PROJECT_ROOT, DATA_DIR, get_line\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set()\n",
    "plt.rcParams['font.family'] = 'IPAPGothic'  # TO-DO: Check detail and decide to remove or not\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CELL_WIDTH = 124\n",
    "DASHED_LINE = get_line(CELL_WIDTH)\n",
    "DOUBLE_DASHED_LINE = get_line(CELL_WIDTH, line_type='=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEKO_FILEPATH = DATA_DIR.joinpath('neko.txt')\n",
    "NEKO_MECAB_FILEPATH = DATA_DIR.joinpath('neko.txt.mecab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4章: 形態素解析\n",
    "\n",
    "夏目漱石の小説『吾輩は猫である』の文章（[neko.txt](http://www.cl.ecei.tohoku.ac.jp/nlp100/data/neko.txt)）を MeCab を使って形態素解析し, その結果を neko.txt.mecab というファイルに保存せよ. このファイルを用いて, 以下の問に対応するプログラムを実装せよ.\n",
    "\n",
    "なお，問題[37](#37), [38](#38), [39](#39)は [matplotlib](http://matplotlib.org/) もしくは [Gnuplot](http://www.gnuplot.info/) を用いるとよい."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with NEKO_FILEPATH.open() as f:\n",
    "    lines = f.read()[1:]  # Ignore the first line '-'    \n",
    "lines = re.sub(r\"[\\n\\u3000]\", r\"\", lines)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MeCab.Tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = m.parse(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with NEKO_MECAB_FILEPATH.open('w') as f:\n",
    "    f.write(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30. 形態素解析結果の読み込み\n",
    "\n",
    "形態素解析結果（neko.txt.mecab）を読み込むプログラムを実装せよ. ただし, 各形態素は表層形（```surface```）, 基本形（```base```）, 品詞（```pos```）, 品詞細分類1（```pos1```）をキーとするマッピング型に格納し, 1文を形態素（マッピング型）のリストとして表現せよ. 第4章の残りの問題では, ここで作ったプログラムを活用せよ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping(line: str) -> dict:\n",
    "    \"\"\"Define a map or dict containing part-of-speech information.\"\"\"\n",
    "    line = line.strip()\n",
    "    tab_splitted = line.split('\\t')\n",
    "    surface = tab_splitted[0]\n",
    "    comma_splitted = tab_splitted[1].split(',')\n",
    "    base, pos, pos1 = comma_splitted[-3], comma_splitted[0], comma_splitted[1]\n",
    "    return {'surface': surface, 'base': base, 'pos': pos, 'pos1': pos1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with NEKO_MECAB_FILEPATH.open() as f:\n",
    "    tagged = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "sent = []\n",
    "for line in tagged[:-1]:  # Ignore EOF symbol\n",
    "    sent += [get_mapping(line)]\n",
    "    if line[0] == '。':\n",
    "        sents += [sent]\n",
    "        sent = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31. 動詞\n",
    "\n",
    "動詞の表層形をすべて抽出せよ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['生れ', 'つか', 'し', '泣い', 'し', 'いる', '始め', '見', '聞く', '捕え', '煮', '食う', '思わ', '載せ', 'られ', '持ち上げ', 'られ', 'し', 'あっ', '落ちつい', '見', '見', '思っ', '残っ', 'いる', 'さ', 'れ', 'し', '逢っ', '出会わ', 'し', 'なら', 'し', 'いる', '吹く', 'せ', '弱っ', '飲む', '知っ', '坐っ', 'おっ', 'する', 'し', '始め', '動く', '動く', '分ら', '廻る', 'なる', '助から', '思っ', 'いる', 'さり', 'し', '出', 'し', 'いる', '考え出そ', '分ら', '付い', '見る', 'い', 'おっ', '見え', '隠し', 'しまっ', '違っ', '明い', 'い', 'られ', '這い出し', '見る', '棄て', 'られ', '這い出す', 'ある', '坐っ', 'し', '考え', '見', '出', 'し', '泣い', '来', 'くれる', '考え付い', 'やっ', '見', '来', '渡っ', 'かかる', '減っ', '来', '泣き', '出', 'ある', 'ある', 'し', 'そろ', '廻り']\n"
     ]
    }
   ],
   "source": [
    "verb_surfaces = [\n",
    "    mapping['surface'] for sent in sents \n",
    "                       for mapping in sent if mapping['pos'] == '動詞'\n",
    "]\n",
    "\n",
    "# Output only the first 100 results\n",
    "print(verb_surfaces[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32. 動詞の原形\n",
    "\n",
    "動詞の原形をすべて抽出せよ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['生れる', 'つく', 'する', '泣く', 'する', 'いる', '始める', '見る', '聞く', '捕える', '煮る', '食う', '思う', '載せる', 'られる', '持ち上げる', 'られる', 'する', 'ある', '落ちつく', '見る', '見る', '思う', '残る', 'いる', 'する', 'れる', 'する', '逢う', '出会う', 'する', 'なる', 'する', 'いる', '吹く', 'する', '弱る', '飲む', '知る', '坐る', 'おる', 'する', 'する', '始める', '動く', '動く', '分る', '廻る', 'なる', '助かる', '思う', 'いる', 'さる', 'する', '出る', 'する', 'いる', '考え出す', '分る', '付く', '見る', 'いる', 'おる', '見える', '隠す', 'しまう', '違う', '明く', 'いる', 'られる', '這い出す', '見る', '棄てる', 'られる', '這い出す', 'ある', '坐る', 'する', '考える', '見る', '出る', 'する', '泣く', '来る', 'くれる', '考え付く', 'やる', '見る', '来る', '渡る', 'かかる', '減る', '来る', '泣く', '出る', 'ある', 'ある', 'する', 'そる', '廻る']\n"
     ]
    }
   ],
   "source": [
    "verb_bases = [\n",
    "    mapping['base'] for sent in sents\n",
    "                    for mapping in sent if mapping['pos'] == '動詞'\n",
    "]\n",
    "\n",
    "# Output only the first 100 results\n",
    "print(verb_bases[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 33. サ変名詞\n",
    "\n",
    "サ変接続の名詞をすべて抽出せよ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['見当', '記憶', '話', '装飾', '突起', '運転', '記憶', '分別', '決心', '我慢', '餓死', '訪問', '始末', '猶予', '遭遇', '我慢', '記憶', '返報', '勉強', '勉強', '昼寝', '珍重', '昼寝', '経験', '供', '供', '供', '供', '*', '*', '同居', '観察', '断言', '同衾', '供', '迫害', '尊敬', '生活', '剿滅', '議論', '所有', '憤慨', '観念', '御馳走', '掠奪', '代言', '我儘', '我儘', '失敗', '話', '投書', '関', '決心', '昼寝', '鑑定', '話', '述懐', '想像', '写生', '感心', '昼寝', '失笑', '揶揄', '写生', '欠伸', '自白', '彩色', '写生', '判然', '心中', '感服', '小便', '猶予', '失敬', '欠伸', '予定', '失望', '漫罵', '小便', '増長', '増長', '我儘', '我慢', '報道', '供', '昼寝', '退屈', '加減', '一睡', '運動', '嘆賞', '佇立', '記憶', '珍重', '身動き', '挨拶', '鼓動', '軽蔑', '察', '肥満']\n"
     ]
    }
   ],
   "source": [
    "nouns_sahen_concated = [\n",
    "    mapping['base'] for sent in sents for mapping in sent \n",
    "                    if mapping['pos'] == '名詞' and mapping['pos1'] == 'サ変接続'\n",
    "]\n",
    "\n",
    "# Output only the first 100 results\n",
    "print(nouns_sahen_concated[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 34. 「AのB」\n",
    "\n",
    "2つの名詞が「の」で連結されている名詞句を抽出せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 35. 名詞の連接\n",
    "\n",
    "名詞の連接（連続して出現する名詞）を最長一致で抽出せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 36. 単語の出現頻度\n",
    "\n",
    "文章中に出現する単語とその出現頻度を求め, 出現頻度の高い順に並べよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 37. 頻度上位10語\n",
    "\n",
    "出現頻度が高い10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 38. ヒストグラム\n",
    "\n",
    "単語の出現頻度のヒストグラム（横軸に出現頻度, 縦軸に出現頻度をとる単語の種類数を棒グラフで表したもの）を描け."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 39. Zipf の法則\n",
    "\n",
    "単語の出現頻度順位を横軸, その出現頻度を縦軸として, 両対数グラフをプロットせよ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
